{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osiris/test/blob/develop/ia-nnip-p02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J9nZra6t-A4"
      },
      "source": [
        "# Práctica 2: Redes neuronales para clasificación de imágenes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta práctica, implementaremos redes neuronales para clasificar dígitos manuscritos sin necesidad de extracción manual de características. Utilizaremos el dataset MNIST, que contiene imágenes de dígitos de 28x28 píxeles.\n",
        "\n"
      ],
      "metadata": {
        "id": "cauS5jv_8z_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para empezar, importamos todas las bibliotecas que necesitaremos:"
      ],
      "metadata": {
        "id": "ra-u8pnK81ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = [8, 6]"
      ],
      "metadata": {
        "id": "HG2H75ImpbMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego, cargamos el dataset MNIST y visualizamos algunos ejemplos del conjunto de entrenamiento:"
      ],
      "metadata": {
        "id": "t1oATViypI2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.MNIST(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    root=\"./data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "plt.figure()\n",
        "for i in range(16):\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    plt.imshow(train_dataset.data[i], cmap=plt.get_cmap(\"gray_r\"))\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "Rk63hhehpHth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Red Neuronal Multicapa"
      ],
      "metadata": {
        "id": "o-JTq7F75IOi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZC9uQ-ivsmz"
      },
      "source": [
        "En primer luegar, implementaremos una red neuronal multicapa (`MultiLayerNN`) para que pueda clasificar imágenes de 28x28 píxeles en 10 categorías diferentes.\n",
        "\n",
        "## Arquitectura de la red\n",
        "* La red contará con dos capas ocultas, cada una con 512 neuronas, y aplicaremos una función de activación ReLU después de cada capa oculta para introducir no linealidad.\n",
        "* La capa de salida tendrá 10 neuronas, lo que corresponde a las 10 posibles categorías (dígitos del 0 al 9) que el modelo debe predecir.\n",
        "\n",
        "## Transformación de los datos de entrada\n",
        "Antes de procesar las imágenes, es necesario convertirlas de una matriz de 28x28 píxeles a un vector de 784 componentes. Esto se logra utilizando la función `view` de los tensores de PyTorch.\n",
        "\n",
        "## Herramientas de PyTorch:\n",
        "Algunas clases y funciones de PyTorch que nos serán útiles para implementar el modelo son:\n",
        "* `nn.Linear`: implementa una capa totalmente conectada. Debemos especificar el número de entradas y salidas de la capa.\n",
        "* `F.relu`: implementa la función de activación ReLU, que se aplica después de las capas ocultas (la podemos aplicar directamente dentro del método `forward` de la red).\n",
        "\n",
        "## Entrenamiento del modelo\n",
        "* **Optimizador**: utilizaremos el método de Gradiente Descendente Estocástico por mini-batches (`optim.SGD`) para ajustar los pesos de la red durante el entrenamiento.\n",
        "* **Función de pérdida**: la función de pérdida será la Entropía Cruzada (`nn.CrossEntropyLoss`), adecuada para problemas de clasificación multiclase, como la clasificación de dígitos manuscritos."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comenzamos implementando la red neuronal multicapa:"
      ],
      "metadata": {
        "id": "4eiK8K45zn94"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSHUXi5u7QmP"
      },
      "source": [
        "class MultiLayerNN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Red neuronal multicapa (MultiLayerNN)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Primera capa oculta\n",
        "        self.fc1 = nn.Linear(..., ...)\n",
        "\n",
        "        # Segunda capa oculta\n",
        "        self.fc2 = nn.Linear(..., ...)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc3 = nn.Linear(..., ...)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Transformar la entrada en un vector para las capas totalmente conectadas\n",
        "        x = x.view(-1, 784)\n",
        "\n",
        "        # Aplicar la primera capa oculta seguida de ReLU\n",
        "        x = self.fc1(...)\n",
        "        x = F.relu(...)\n",
        "\n",
        "        # Aplicar la segunda capa oculta seguida de ReLU\n",
        "        x = self.fc2(...)\n",
        "        x = F....\n",
        "\n",
        "        # Aplicar la capa de salida\n",
        "        x = self.fc3(...)\n",
        "\n",
        "        return ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, implementamos el proceso de entrenamiento de la red:"
      ],
      "metadata": {
        "id": "IAp9hwzrz2aI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración del dispositivo (GPU si está disponible, de lo contrario CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Construcción del modelo y envío al dispositivo\n",
        "model1 = ...\n",
        "model1.to(device)\n",
        "\n",
        "# Creación de dataLoaders para manejar los batches de entrenamiento y prueba\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Función de pérdida y optimizador\n",
        "criterion = ...\n",
        "optimizer = optim.SGD(model1.parameters(), lr=0.01)\n",
        "\n",
        "# Lista para guardar los valores de pérdida durante el entrenamiento\n",
        "train_loss = []\n",
        "\n",
        "# Épocas de entrenamiento\n",
        "num_epochs = 10\n",
        "\n",
        "# Establecer el modelo en modo entrenamiento\n",
        "model1.train()\n",
        "\n",
        "# Bucle de entrenamiento\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        # Mover los datos a la GPU\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Limpiar los gradientes\n",
        "        optimizer....\n",
        "\n",
        "        # Pasada forward\n",
        "        outputs = model1(...)\n",
        "\n",
        "        # Cálcular la pérdida\n",
        "        loss = ...\n",
        "\n",
        "        # Pasada backward\n",
        "        loss....\n",
        "\n",
        "        # Actualizar los pesos\n",
        "        optimizer....\n",
        "\n",
        "        # Guardar el valor de pérdida\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "    print(f\"Época {epoch + 1}/{num_epochs}, Pérdida: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "JF9FAny3xghZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardamos el modelo para poder reutilizarlo cuando queramos:"
      ],
      "metadata": {
        "id": "5DgpIJFT0B4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model1.state_dict(), \"./multinn.pth\")"
      ],
      "metadata": {
        "id": "yUOx_HufzT39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizamos los valores de la función de pérdida durante el entrenamiento:"
      ],
      "metadata": {
        "id": "jrbH96t00FCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = np.array(train_loss)\n",
        "N = 60\n",
        "run_avg_train_loss = np.convolve(train_loss, np.ones((N,))/N, mode=\"valid\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_loss, alpha = 0.3) # original\n",
        "plt.plot(run_avg_train_loss, color=\"red\")   # versión suavizada\n",
        "plt.title(\"Pérdida durante el entrenamiento (MultiLayerNN)\")"
      ],
      "metadata": {
        "id": "hrvbDeG4ytPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos una función para evaluar el modelo entrenado en el conjunto de prueba:"
      ],
      "metadata": {
        "id": "Iyou-jpk0frn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy(model, test_dataset, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Calcula la accuracy general y por dígito en el conjunto de prueba.\n",
        "    \"\"\"\n",
        "    model.to(...)\n",
        "\n",
        "    # Establecer el modelo en modo evaluación\n",
        "    model.eval()\n",
        "\n",
        "    correct_per_class = torch.zeros(10)  # Conteo correcto por dígito\n",
        "    total_per_class = torch.zeros(10)    # Total de ejemplos por dígito\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "    with torch.no_grad():  # No necesitamos gradientes durante la evaluación\n",
        "        for images, labels in test_loader:\n",
        "            # Mover los datos a la GPU\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Pasada forward\n",
        "            outputs = model(...)\n",
        "\n",
        "            # Obtener las predicciones\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Actualizar el conteo de predicciones correctas y totales por dígito\n",
        "            for i in range(10):\n",
        "                correct_per_class[i] += (predicted[labels == i] == i).sum().item()\n",
        "                total_per_class[i] += (labels == i).sum().item()\n",
        "\n",
        "            # Actualizar el conteo de predicciones correctas y totales en general\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    # Calcular la accuracy general\n",
        "    overall_accuracy = total_correct / ...\n",
        "\n",
        "    # Calcular la accuracy por dígito\n",
        "    accuracy_per_digit = correct_per_class / ...\n",
        "\n",
        "    return overall_accuracy, accuracy_per_digit.tolist()"
      ],
      "metadata": {
        "id": "SvPZC2AAkekz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos el modelo entrenado y lo evaluamos en conjunto de prueba:"
      ],
      "metadata": {
        "id": "b2Ti_Wml1Wjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo entrenado (no es necesario si se encuentra en memoria)\n",
        "model1 = ...\n",
        "model1.load_state_dict(torch.load(\"./multinn.pth\"))\n",
        "model1.to(...)\n",
        "\n",
        "# Evaluar el modelo entrenado en el conjunto de prueba\n",
        "overall_accuracy, accuracy_per_digit = evaluate_accuracy(..., ..., device)\n",
        "\n",
        "print(\"Resultados MultiLayerNN:\\n\")\n",
        "\n",
        "# Mostrar la accuracy general\n",
        "print(f\"Accuracy General: {overall_accuracy:.4f}\")\n",
        "print()\n",
        "\n",
        "# Mostrar la accuracy por dígito\n",
        "for i, acc in enumerate(accuracy_per_digit):\n",
        "    print(f\"Accuracy para el dígito {i}: {acc:.4f}\")"
      ],
      "metadata": {
        "id": "1MEZSQpc1V3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Red Neuronal Convolucional"
      ],
      "metadata": {
        "id": "s1NXz5-H5Ttf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qQYQziCx5fz"
      },
      "source": [
        "Ahora probaremos resolver el problema de clasificación utilizando una red neuronal convolucional (`CNN`).\n",
        "\n",
        "## Arquitectura de la red\n",
        "La arquitectura de la red está compuesta por las siguientes capas:\n",
        "\n",
        "* Capa convolucional 1: 6 filtros de 5x5 con stride de 1 y padding de 2.\n",
        "* Función de activación ReLU.\n",
        "* Max pooling 1: filtro de 2x2 con stride de 2 y sin padding.\n",
        "* Capa convolucional 2: 16 filtros de 5x5 con stride de 1 y sin padding.\n",
        "* Función de activación ReLU.\n",
        "* Max pooling 2: filtro de 2x2 con stride de 2 y sin padding.\n",
        "* Capa totalmente conectada 1: 120 neuronas.\n",
        "* Función de activación ReLU.\n",
        "* Capa totalmente conectada 2: 84 neuronas.\n",
        "* Función de activación ReLU.\n",
        "* Capa de salida: 10 neuronas.\n",
        "\n",
        "## Herramientas de PyTorch:\n",
        "Algunas clases y funciones de PyTorch que nos serán útiles para implementar el modelo son:\n",
        "* `nn.Conv2D`: una capa convolucional 2D. Es necesario especificar el número de canales de entrada, el número de filtros, el tamaño del kernel, el stride y el padding.\n",
        "* `nn.MaxPool2d`: implementa una capa de max pooling 2D. Se especifica el tamaño del filtro de pooling y el stride.\n",
        "* `nn.Linear`: implementa una capa totalmente conectada. Debemos especificar el número de entradas y salidas de la capa.\n",
        "* `F.relu`: implementa la función de activación ReLU, que se aplica después de las capas de convolución y totalmente conectadas (la podemos aplicar directamente dentro del método `forward` de la red).\n",
        "\n",
        "## Entrenamiento del modelo\n",
        "* **Optimizador**: utilizaremos el método de Gradiente Descendente Estocástico por mini-batches (`optim.SGD`) para ajustar los pesos de la red durante el entrenamiento.\n",
        "* **Función de pérdida**: la función de pérdida será la Entropía Cruzada (`nn.CrossEntropyLoss`), adecuada para problemas de clasificación multiclase, como la clasificación de dígitos manuscritos.\n",
        "\n",
        "## Salida de las capas convolucionales:\n",
        "Para calcular el tamaño de salida de las capas convolucionales, utilizaremos la siguiente fórmula:\n",
        "\n",
        "$$O = \\frac{(W - F + 2P)}{S} + 1$$\n",
        "\n",
        "Donde:\n",
        "* F: tamaño del filtro.\n",
        "* W: tamaño de la entrada.\n",
        "* P: padding aplicado.\n",
        "* S: stride utilizado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementamos la red neuronal convolucional:"
      ],
      "metadata": {
        "id": "HIOXeWJE3k59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Red neuronal convolucional (CNN)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Primera capa convolucional + Max pooling\n",
        "        self.conv1 = nn.Conv2d(in_channels=..., out_channels=..., kernel_size=..., stride=..., padding=...)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=..., stride=...)\n",
        "\n",
        "        # Segunda capa convolucional + Max pooling\n",
        "        self.conv2 = nn.Conv2d...\n",
        "        self.pool2 = nn.MaxPool2d...\n",
        "\n",
        "        # Capas totalmente conectadas 1 y 2\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, ...)\n",
        "        self.fc2 = nn.Linear(..., ...)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc3 = nn.Linear(..., ...)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Aplicar la primera capa convolucional seguida de ReLU y Max Pooling\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(...)\n",
        "        x = self.pool1(...)\n",
        "\n",
        "        # Aplicar la segunda capa convolucional seguida de ReLU y Max Pooling\n",
        "        x = ...\n",
        "        x = ...\n",
        "        x = ...\n",
        "\n",
        "        # Transformar la entrada en un vector para las capas totalmente conectadas\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "\n",
        "        # Aplicar la primera capa totalmente conectada seguida de ReLU\n",
        "        x = self.fc1(...)\n",
        "        x = torch.relu(...)\n",
        "\n",
        "        # Aplicar la segunda capa totalmente conectada seguida de ReLU\n",
        "        x = self.fc2(...)\n",
        "        x = torch.relu(...)\n",
        "\n",
        "        # Aplicar la capa de salida\n",
        "        x = self.fc3(...)\n",
        "\n",
        "        return ..."
      ],
      "metadata": {
        "id": "uNs6Q6AFgEqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y el proceso de entrenamiento:"
      ],
      "metadata": {
        "id": "taSTUfkH3phK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración del dispositivo (GPU si está disponible, de lo contrario CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Construcción del modelo y envío al dispositivo\n",
        "model2 = ...\n",
        "model2.to(device)\n",
        "\n",
        "# Creación de dataLoaders para manejar los batches de entrenamiento y prueba\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Función de pérdida y optimizador\n",
        "criterion = ...\n",
        "optimizer = optim.SGD(model2.parameters(), lr=0.01)\n",
        "\n",
        "# Lista para guardar los valores de pérdida durante el entrenamiento\n",
        "train_loss = []\n",
        "\n",
        "# Épocas de entrenamiento\n",
        "num_epochs = 10\n",
        "\n",
        "# Establecer el modelo en modo entrenamiento\n",
        "model2.train()\n",
        "\n",
        "# Bucle de entrenamiento\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        # Mover los datos a la GPU\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Limpiar los gradientes\n",
        "        optimizer....\n",
        "\n",
        "        # Pasada forward\n",
        "        outputs = ...\n",
        "\n",
        "        # Cálcular la pérdida\n",
        "        loss = ...\n",
        "\n",
        "        # Pasada backward\n",
        "        loss.....\n",
        "\n",
        "        # Actualizar los pesos\n",
        "        optimizer.....\n",
        "\n",
        "        # Guardar el valor de pérdida\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "    print(f\"Época {epoch + 1}/{num_epochs}, Pérdida: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "PlCieWljo1Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardamos el modelo para poder reutilizarlo cuando queramos:"
      ],
      "metadata": {
        "id": "9G2-T-pN4DNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model2.state_dict(), \"./cnn.pth\")"
      ],
      "metadata": {
        "id": "9DQ1fZpb4DNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizamos los valores de la función de pérdida durante el entrenamiento:"
      ],
      "metadata": {
        "id": "MbIlJW4M4DN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = np.array(train_loss)\n",
        "N = 60\n",
        "run_avg_train_loss = np.convolve(train_loss, np.ones((N,))/N, mode=\"valid\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_loss, alpha = 0.3) # original\n",
        "plt.plot(run_avg_train_loss, color=\"red\")   # versión suavizada\n",
        "plt.title(\"Pérdida durante el entrenamiento (CNN)\")"
      ],
      "metadata": {
        "id": "0WWoyYEb4DN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos el modelo entrenado y lo evaluamos en conjunto de prueba:"
      ],
      "metadata": {
        "id": "-wDe3PY94Iep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo entrenado (no es necesario si se encuentra en memoria)\n",
        "model2 = CNN()\n",
        "model2.load_state_dict(torch.load(\"./cnn.pth\"))\n",
        "model2.to(device)\n",
        "\n",
        "# Evaluar el modelo entrenado en el conjunto de prueba\n",
        "overall_accuracy, accuracy_per_digit = evaluate_accuracy(..., ..., device)\n",
        "\n",
        "print(\"Resultados CNN:\\n\")\n",
        "\n",
        "# Mostrar la accuracy general\n",
        "print(f\"Accuracy General: {overall_accuracy:.4f}\")\n",
        "print()\n",
        "\n",
        "# Mostrar la accuracy por dígito\n",
        "for i, acc in enumerate(accuracy_per_digit):\n",
        "    print(f\"Accuracy para el dígito {i}: {acc:.4f}\")"
      ],
      "metadata": {
        "id": "3aBBomml4Ie4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entregable"
      ],
      "metadata": {
        "id": "PcleJKAV5ahV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ee9CYVb9B22"
      },
      "source": [
        "1. Completar el código para la red neuronal multicapa (MultiLayerNN) y la red neuronal convolucional (CNN).\n",
        "2. Analizar los resultados obtenidos en el conjunto de prueba para ambas redes.\n",
        "\n",
        "(Opcional):\n",
        "3. Calcular la cantidad total de parámetros en cada red neuronal y comparar estos valores.\n",
        "4. Para la red convolucional, calcular y documentar el tamaño de las entradas y salidas de cada capa convolucional, así como el número de feature maps generados en cada capa.\n"
      ]
    }
  ]
}