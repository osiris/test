{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osiris/test/blob/develop/Pr%C3%A1ctica_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5_c-yz8pLHE"
      },
      "source": [
        "# **Práctica 3: Segmentación de Imágenes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOkEIfmHngPu"
      },
      "source": [
        "En esta práctica, implementaremos y entrenaremos un modelo U-Net para la tarea de segmentación de imágenes de radiografías de tórax, enfocándonos en segmentación de las estructuras pulmonares.\n",
        "\n",
        "El modelo U-Net es ampliamente utilizado en la segmentación de imágenes médicas debido a su capacidad para capturar tanto características de bajo nivel como de alto nivel. Para obtener más información sobre el modelo U-Net, consultar el paper original disponible en: [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BugHCbL3sgBs"
      },
      "source": [
        "Para comenzar, importamos todas las bibliotecas que necesitaremos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVbJgrpJskBo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkZoIGyepY5u"
      },
      "source": [
        "## **Dataset JSRT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjUwFtsApHZZ"
      },
      "source": [
        "Utilizaremos el dataset [JSRT](http://db.jsrt.or.jp/eng.php), que contiene imágenes de radiografías de tórax y sus correspondientes máscaras de segmentación de los pulmones.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgcjiKBbr8qQ"
      },
      "source": [
        "En primer lugar, **descargá la versión reducida del dataset**, que contiene imágenes en resolución 256x256, desde [este enlace](https://drive.google.com/file/d/1-faPhIl7tWxC0AgWx4zmnm1iP5J2YBwu/view?usp=sharing).\n",
        "\n",
        "Una vez completada la descarga, **descomprimí el archivo .zip y ubicá la carpeta \"Segmentation01\" en el directorio principal de tu Google Drive**. Así, la estructura de archivos debería quedar de la siguiente manera:\n",
        "\n",
        "```\n",
        "Mi Unidad/\n",
        "    Segmentation01/\n",
        "        train/\n",
        "        test/\n",
        "        list_train.txt\n",
        "        list_test.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de acceder a los datos, es necesario **montar tu Google Drive en el entorno de Google Colab**.\n",
        "\n",
        "Para ello, ejecutá la celda de código que sigue y seguí las instrucciones:"
      ],
      "metadata": {
        "id": "dntbgxfFxK2t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbJONQNoc7q5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09ZqEmSMrjN0"
      },
      "source": [
        "Una vez montado el Drive, podés acceder a la carpeta \"Segmentation01\" usando la siguiente ruta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5IVnj9UsaoJ"
      },
      "outputs": [],
      "source": [
        "root_dir = \"/content/drive/My Drive/Segmentation01/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqajf2z3O_V7"
      },
      "source": [
        "## **Lectura, partición y visualización de datos**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkZdlqWJtdXO"
      },
      "source": [
        "En esta parte, utilizaremos la función `load_image_mask_paths` para leer y cargar las rutas de las imágenes y máscaras correspondientes desde los archivos `list_train.txt` y `list_test.txt`. Estos archivos contienen la **lista de imágenes y máscaras que se utilizarán para entrenamiento y prueba**, respectivamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khJktEtLZFKD"
      },
      "outputs": [],
      "source": [
        "def load_image_mask_paths(file_path):\n",
        "    \"\"\"\n",
        "    Lee un archivo y devuelve una lista de tuplas donde cada tupla\n",
        "    contiene la ruta de una imagen y su correspondiente máscara.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Ruta al archivo que contiene las rutas de las imágenes y máscaras.\n",
        "\n",
        "    Returns:\n",
        "        list of tuples: Lista de tuplas (image_path, mask_path).\n",
        "    \"\"\"\n",
        "    image_mask_paths = []\n",
        "    with open(file_path, \"r\") as file:\n",
        "        for line in file:\n",
        "            # Limpia la línea eliminando espacios y reemplaza las barras invertidas con barras normales\n",
        "            cleaned_line = line.strip().replace(\"\\\\\", \"/\")\n",
        "\n",
        "            # Elimina el prefijo \"./\" si está presente al inicio de la línea\n",
        "            if cleaned_line.startswith(\"./\"):\n",
        "                cleaned_line = cleaned_line[2:]\n",
        "\n",
        "            # Divide la línea en ruta de imagen y ruta de máscara, y las agrega a la lista\n",
        "            image_path, mask_path = cleaned_line.split(\",\")\n",
        "            image_mask_paths.append((image_path, mask_path))\n",
        "\n",
        "    return image_mask_paths\n",
        "\n",
        "\n",
        "# Rutas de los archivos que contienen las listas de imágenes y máscaras para entrenamiento y prueba\n",
        "train_files = os.path.join(root_dir, \"list_train.txt\")\n",
        "test_files = os.path.join(root_dir, \"list_test.txt\")\n",
        "\n",
        "# Leemos las rutas de las imágenes y máscaras para entrenamiento y prueba\n",
        "train_image_mask_paths = load_image_mask_paths(train_files)\n",
        "test_image_mask_paths = load_image_mask_paths(test_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjykYjAUMpVu"
      },
      "source": [
        "A continuación, dividimos el conjunto de entrenamiento en dos subconjuntos, **uno para entrenamiento y otro para validación**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9OZN1mPM1lY"
      },
      "outputs": [],
      "source": [
        "# Definimos el tamaño del conjunto de validación\n",
        "val_samples = len(test_image_mask_paths)\n",
        "\n",
        "# Tamaño del conjunto de entrenamiento\n",
        "train_samples = len(train_image_mask_paths)\n",
        "\n",
        "# Generamos índices aleatorios para el conjunto de validación\n",
        "val_indices = random.sample(range(train_samples), val_samples)\n",
        "\n",
        "# Separamos los índices en conjuntos de entrenamiento y validación\n",
        "val_image_mask_paths = [train_image_mask_paths[i] for i in val_indices]\n",
        "train_image_mask_paths = [train_image_mask_paths[i] for i in range(train_samples) if i not in val_indices]\n",
        "\n",
        "\n",
        "# Imprimimos la cantidad de ejemplos de entrenamiento, validación y prueba\n",
        "print(f\"Cantidad de ejemplos de entrenamiento: {len(train_image_mask_paths)}\")\n",
        "print(f\"Cantidad de ejemplos de validación: {len(val_image_mask_paths)}\")\n",
        "print(f\"Cantidad de ejemplos de prueba: {len(test_image_mask_paths)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix2cmtZkCdkm"
      },
      "source": [
        "Por último, visualizamos algunos ejemplos del conjunto de entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAuPNvDEhDtK"
      },
      "outputs": [],
      "source": [
        "# Cantidad de ejemplos a visualizar\n",
        "num_samples_to_display = 2\n",
        "num_samples_to_display = min(num_samples_to_display, len(train_image_mask_paths))\n",
        "\n",
        "plt.figure(figsize=(9, num_samples_to_display * 3))\n",
        "\n",
        "for i, (image_path, mask_path) in enumerate(train_image_mask_paths[:num_samples_to_display]):\n",
        "    # Cargamos la imagen y la máscara\n",
        "    image = Image.open(os.path.join(root_dir, image_path)).convert(\"RGB\")\n",
        "    mask = Image.open(os.path.join(root_dir, mask_path)).convert(\"L\")\n",
        "\n",
        "    # Convertimos la máscara a un array de numpy y normalizamos\n",
        "    mask_array = np.array(mask)\n",
        "    mask_color = np.zeros((*mask_array.shape, 3), dtype=np.uint8)\n",
        "    mask_color[mask_array > 0] = [255, 0, 0]  # Color rojo para la máscara\n",
        "\n",
        "    # Convertimos a imágenes PIL\n",
        "    mask_color_image = Image.fromarray(mask_color)\n",
        "\n",
        "    # Superponemos la máscara sobre la imagen\n",
        "    superimposed_image = Image.blend(image, mask_color_image, alpha=0.3)\n",
        "\n",
        "    # Muestra la imagen original\n",
        "    plt.subplot(num_samples_to_display, 3, i * 3 + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"Imagen: {os.path.basename(image_path)}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Muestra la máscara\n",
        "    plt.subplot(num_samples_to_display, 3, i * 3 + 2)\n",
        "    plt.imshow(mask, cmap=\"gray\")\n",
        "    plt.title(f\"Máscara: {os.path.basename(mask_path)}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Muestra la superposición de la máscara sobre la imagen\n",
        "    plt.subplot(num_samples_to_display, 3, i * 3 + 3)\n",
        "    plt.imshow(superimposed_image)\n",
        "    plt.title(f\"Superposición\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaKcYMOYu9_f"
      },
      "source": [
        "## **Definición de un dataset personalizado en PyTorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSw685eKFArd"
      },
      "source": [
        "Lo que haremos a continuación, será **crear una clase personalizada en PyTorch para manejar y procesar los datos del conjunto JSRT**. Esta clase nos permitirá cargar las imágenes y máscaras, aplicar transformaciones para aumentación de datos, y preparar los datos para el entrenamiento del modelo.\n",
        "\n",
        "PyTorch proporciona la clase `torch.utils.data.Dataset`, que nos permite crear datasets a medida. Nuestra clase heredará de esta y nos permitirá controlar cómo se cargan los datos y cómo se aplican las transformaciones.\n",
        "\n",
        "Para obtener más información sobre cómo crear y utilizar datasets en PyTorch, podés consultar la [documentación oficial](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) y algunos tutoriales útiles como [éste](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1U4ghGiXwcw"
      },
      "outputs": [],
      "source": [
        "class JSRTDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset personalizado para el conjunto de datos JSRT.\n",
        "\n",
        "    Args:\n",
        "        root_dir (str): Directorio raíz donde se encuentran las imágenes y máscaras.\n",
        "        image_mask_paths (list of tuples): Lista de tuplas (image_path, mask_path) con las rutas a las imágenes y máscaras.\n",
        "        is_train (bool): Indica si el dataset se utiliza para entrenamiento. Si es True, se aplican aumentaciones. Si es False, no se aplican.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, image_mask_paths, is_train=True):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_mask_paths = image_mask_paths\n",
        "        self.is_train = is_train\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Devuelve el número total de ejemplos en el dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Número de ejemplos en el dataset.\n",
        "        \"\"\"\n",
        "        return len(self.image_mask_paths)\n",
        "\n",
        "    def transform(self, image, mask):\n",
        "        \"\"\"\n",
        "        Aplica las transformaciones a un ejemplo del dataset.\n",
        "\n",
        "        Args:\n",
        "            image (PIL.Image): Imagen a transformar.\n",
        "            mask (PIL.Image): Máscara a transformar.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Imagen y máscara transformadas.\n",
        "        \"\"\"\n",
        "        # Aplicamos reflejo horizontal\n",
        "        if random.random() > 0.5:\n",
        "            image = TF.hflip(image)\n",
        "            mask = TF.hflip(mask)\n",
        "\n",
        "        # Aplicamos rotación\n",
        "        if random.random() > 0.5:\n",
        "            angle = int(random.uniform(-5, 5))  # Ángulo de rotación aleatorio entre -5 y 5 grados\n",
        "            image = TF.rotate(image, angle)\n",
        "            mask = TF.rotate(mask, angle)\n",
        "\n",
        "        # Convertimos a tensor\n",
        "        image = TF.to_tensor(image)\n",
        "        mask = TF.to_tensor(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Obtiene una muestra del dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Índice de la muestra a obtener.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Imagen y máscara en el índice proporcionado, después de aplicar las transformaciones (si las hay).\n",
        "        \"\"\"\n",
        "        image_path, mask_path = self.image_mask_paths[idx]\n",
        "\n",
        "        # Cargamos la imagen y la máscara\n",
        "        image = Image.open(os.path.join(self.root_dir, image_path)).convert(\"L\")\n",
        "        mask = Image.open(os.path.join(self.root_dir, mask_path)).convert(\"L\")\n",
        "\n",
        "        # Aplicamos las transformaciones si el dataset está en modo entrenamiento\n",
        "        if self.is_train:\n",
        "            image, mask = self.transform(image, mask)\n",
        "\n",
        "        # Nos aseguramos que la imagen y la máscara sean tensores\n",
        "        if not isinstance(image, torch.Tensor):\n",
        "            image = TF.to_tensor(image)\n",
        "        if not isinstance(mask, torch.Tensor):\n",
        "            mask = TF.to_tensor(mask)\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_707wOaZSSkp"
      },
      "source": [
        "## **Modelo U-Net para segmentación**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UJOXh6Yw1h3"
      },
      "source": [
        "A continuación, procederemos a definir el modelo U-Net que utilizaremos para segmentar imágenes de radiografías de tórax.\n",
        "\n",
        "Puntos clave a tener en cuenta:\n",
        "\n",
        "- **Convoluciones:** para simplificar la implementación, utilizar convoluciones que mantengan el tamaño de la entrada. Esto se puede lograr mediante convoluciones 2D con un kernel de 3x3, padding de 1 y stride de 1.\n",
        "\n",
        "- **Estructura del Encoder:** el encoder de la U-Net deberá tener 4 bloques, cada uno compuesto por dos capas de convolución (usar la función `double_conv`) seguidas de una operación de max pooling (2x2) para reducir las dimensiones espaciales.\n",
        "\n",
        "- **Estructura del Bottleneck:** en el cuello de botella (bottleneck), aplicar dos capas de convolución adicionales (usar la función `double_conv`) sin max pooling, para capturar características de mayor profundidad sin reducir más las dimensiones espaciales.\n",
        "\n",
        "- **Estructura del Decoder:** el decoder deberá contener 4 bloques, cada uno comenzando con una convolución transpuesta para aumentar la dimensión espacial, seguida de dos capas de convolución (usar la función `double_conv`).\n",
        "\n",
        "- **Skip connections:** para implementar las skip connections, utilizar el método `torch.cat` para concatenar los feature maps correspondientes del encoder y el decoder en cada bloque.\n",
        "\n",
        "- **Función de activación:** utilizaremos la función de activación ReLU (`torch.nn.ReLU()`) en todas las capas convolucionales.\n",
        "\n",
        "- **Cantidad de feature maps:**\n",
        "  - **Bloque 1:** 32 feature maps\n",
        "  - **Bloque 2:** 64 feature maps\n",
        "  - **Bloque 3:** 128 feature maps\n",
        "  - **Bloque 4:** 256 feature maps\n",
        "\n",
        "- **Capa de salida:** en la última capa del decoder, aplicar una convolución con un kernel de 1x1 para transformar los últimos 32 feature maps en un solo feature map, correspondiente al mapa de segmentación final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MckifUnCSlrQ"
      },
      "outputs": [],
      "source": [
        "def double_conv(in_channels, features):\n",
        "    \"\"\"\n",
        "    Realiza dos capas de convolución seguidas de activaciones ReLU.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Número de canales de entrada.\n",
        "        features (int): Número de canales de salida.\n",
        "\n",
        "    Returns:\n",
        "        nn.Sequential: Secuencia de dos capas convolucionales y activaciones ReLU.\n",
        "    \"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, features, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(features, features, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementación simplificada de la arquitectura U-Net para segmentación de imágenes.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Número de canales de entrada. Por defecto es 3.\n",
        "        out_channels (int): Número de canales de salida. Por defecto es 1.\n",
        "        init_features (int): Número inicial de características. Por defecto es 32.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder: cuatro bloques con capas convolucionales + pooling\n",
        "        self.encoder1 = double_conv(in_channels, 32)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = double_conv(32, ...)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = double_conv(64, ...)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = double_conv(128, ...)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Bottleneck: parte central de la U-Net que conecta el encoder con el decoder\n",
        "        self.bottleneck = double_conv(..., 512)\n",
        "\n",
        "        # Decoder: cuatro bloques con capas de convolución transpuesta + convolución\n",
        "        self.upconv4 = nn.ConvTranspose2d(512, ..., kernel_size=2, stride=2)\n",
        "        self.decoder4 = double_conv(256 * 2, 256)\n",
        "        self.upconv3 = nn.ConvTranspose2d(256, ..., kernel_size=2, stride=2)\n",
        "        self.decoder3 = double_conv(128 * 2, 128)\n",
        "        self.upconv2 = nn.ConvTranspose2d(128, ..., kernel_size=2, stride=2)\n",
        "        self.decoder2 = double_conv(64 * 2, 64)\n",
        "        self.upconv1 = nn.ConvTranspose2d(64, ..., kernel_size=2, stride=2)\n",
        "        self.decoder1 = double_conv(32 * 2, 32)\n",
        "\n",
        "        # Capa final de convolución para reducir los canales a la cantidad de salida deseada\n",
        "        self.conv = nn.Conv2d(..., out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Realiza la pasada hacia adelante por la U-Net.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Tensor de entrada con dimensiones (batch_size, in_channels, H, W).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor de salida con dimensiones (batch_size, out_channels, H, W).\n",
        "        \"\"\"\n",
        "        # Encoder\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        enc3 = self.encoder3(self.pool2(...))\n",
        "        enc4 = self.encoder4(self.pool3(...))\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(self.pool4(...))\n",
        "\n",
        "        # Decoder con concatenación y skip connections\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)  # Concatenación con la salida del encoder (skip connection)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "        dec3 = self.upconv3(...)\n",
        "        dec3 = torch.cat((..., ...), dim=1)  # Concatenación con la salida del encoder (skip connection)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "        dec2 = self.upconv2(...)\n",
        "        dec2 = torch.cat((..., ...), dim=1)  # Concatenación con la salida del encoder (skip connection)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "        dec1 = self.upconv1(...)\n",
        "        dec1 = torch.cat((..., ...), dim=1)  # Concatenación con la salida del encoder (skip connection)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "\n",
        "        # Capa de salida\n",
        "        y = self.conv(...)\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceLiZAzN0IcF"
      },
      "source": [
        "## **Implementación del entrenamiento**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haFHUKjNVT2m"
      },
      "source": [
        "En esta etapa, implementaremos el proceso de entrenamiento para el modelo de segmentación U-Net.\n",
        "\n",
        "Detalles del entrenamiento:\n",
        "\n",
        "- **Optimizador:** utilizaremos el optimizador **Adam** (podés consultar el paper [acá](https://arxiv.org/abs/1412.6980)), una variante del algoritmo de descenso de gradiente estocástico (SGD). Adam ajusta dinámicamente la tasa de aprendizaje durante el entrenamiento.\n",
        "\n",
        "- **Función de pérdida:** la función de pérdida será **entropía cruzada binaria** (`nn.BCEWithLogitsLoss`). Esta función es ideal para tareas de segmentación binaria ya que combina la activación sigmoide y la entropía cruzada en una sola operación, simplificando así el cálculo.\n",
        "\n",
        "- **Tasa de aprendizaje:** la tasa de aprendizaje inicial será de **0.001** (1e-3). Este valor se puede ajustar durante el entrenamiento si se observa que el modelo converge demasiado rápido o demasiado lento.\n",
        "\n",
        "- **Regularización:** aplicaremos un **weight decay** de **0.00001** (1e-5). Esta técnica ayuda a prevenir el sobreajuste al añadir una penalización al tamaño de los pesos del modelo, promoviendo la generalización.\n",
        "\n",
        "- **Tamaño del batch:** utilizaremos un tamaño de batch de **4 ejemplos**. Este tamaño puede ser modificado en función de la capacidad de la GPU y de los recursos computacionales disponibles.\n",
        "\n",
        "- **Número de épocas:** el modelo se entrenará durante **50 épocas**. Si se dispone de una GPU en el entorno de Colab, este número puede incrementarse a **200 épocas** para mejorar la convergencia y el rendimiento del modelo.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK19tpuRzcxC"
      },
      "source": [
        "Primero, definimos el modelo, la función de pérdida y el optimizador:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhH0VrdLzlEb"
      },
      "outputs": [],
      "source": [
        "# Configuramos del dispositivo (GPU si está disponible, de lo contrario CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Creamos el modelo y los mandamos al dispositivo\n",
        "model = UNet(in_channels=1, out_channels=1)\n",
        "model.to(...)\n",
        "\n",
        "# Función de pérdida y optimizador\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=..., weight_decay=...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpDFzaZ60-38"
      },
      "source": [
        "A continuación, configuraremos los datasets para entrenamiento, validación y prueba, así como los dataloaders correspondientes para manejar los batches de datos.\n",
        "\n",
        "Recordemos que:\n",
        "- El conjunto de datos de **entrenamiento** se utiliza para **entrenar el modelo**.\n",
        "- El conjunto de datos de **validación** se utiliza para **validar el rendimiento del modelo durante el entrenamiento**.\n",
        "- El conjunto de datos de **prueba**  se utiliza para **evaluar el rendimiento final del modelo después del entrenamiento**.\n",
        "\n",
        "\n",
        "Los dataloaders para el conjunto de entrenamiento manejarán los batches de datos con un tamaño especificado (en nuestro caso, 4 ejemplos por batch). **Para los conjuntos validación y prueba, fijaremos el tamaño de los batches a 1**. Esto nos va a permitir evaluar las imágenes individualmente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGMO32Au1O7C"
      },
      "outputs": [],
      "source": [
        "# Creamos las instancias de los datasets para entrenamiento, validación y prueba\n",
        "train_dataset = JSRTDataset(root_dir, train_image_mask_paths, is_train=True)\n",
        "val_dataset = JSRTDataset(root_dir, val_image_mask_paths, is_train=False)\n",
        "test_dataset = JSRTDataset(root_dir, test_image_mask_paths, is_train=False)\n",
        "\n",
        "# Creamos los dataLoaders para manejar los batches de entrenamiento, validación y prueba\n",
        "batch_size = 4\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=1, shuffle=False)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzOKgSa14Nom"
      },
      "source": [
        "A continuación, **implementamos el proceso de entrenamiento del modelo**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6KyDEvTWZUC"
      },
      "outputs": [],
      "source": [
        "# Lista para guardar los valores de pérdida durante el entrenamiento\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "# Épocas de entrenamiento\n",
        "num_epochs = ...\n",
        "\n",
        "# Inicializamos la pérdida de validación para monitoreo\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "# Bucle de entrenamiento\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Entrenamiento\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(...)\n",
        "        loss = criterion(..., ...)\n",
        "\n",
        "        optimizer....\n",
        "        loss....\n",
        "        optimizer....\n",
        "\n",
        "        running_train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
        "    train_loss.append(epoch_train_loss)\n",
        "\n",
        "    # Validación\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(...)\n",
        "            loss = criterion(..., ...)\n",
        "\n",
        "            running_val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
        "    val_loss.append(epoch_val_loss)\n",
        "\n",
        "    print(f\"Época {epoch + 1}/{num_epochs}, Pérdida de entrenamiento: {epoch_train_loss:.4f}, Pérdida de validación: {epoch_val_loss:.4f}\")\n",
        "\n",
        "    # Guardamos el modelo si la pérdida de validación mejora\n",
        "    if epoch_val_loss < ...:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        torch.save(model.state_dict(), \"model_best.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeqZa0XqTuYW"
      },
      "source": [
        "Visualizamos las curvas de entrenamiento y validación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfGb4hMFT__v"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(train_loss, label=\"Entrenamiento\", color=\"blue\")\n",
        "plt.plot(val_loss, label=\"Validación\", color=\"red\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Pérdida\")\n",
        "plt.title(\"Curvas de entrenamiento y validación\")\n",
        "plt.legend()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnGAAoj-4UBa"
      },
      "source": [
        "## **Evaluación del modelo en el conjunto de prueba**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhNKUvQTYHcy"
      },
      "source": [
        "Habiendo entrenado el modelo, **vamos a evaluar su desempeño utilizando el conjunto de prueba**.\n",
        "\n",
        "Para evaluar la calidad de las segmentaciones que el modelo predice, utilizaremos dos métricas:\n",
        "\n",
        "- **Accuracy (Exactitud):** medimos la exactitud del modelo en la clasificación de cada píxel, calculando la proporción de píxeles correctamente clasificados en relación con el total de píxeles.\n",
        "\n",
        "- **Dice Coefficient (Coeficiente de Dice):** calculamos el coeficiente de Dice, midiendo la superposición entre la máscara predicha y la máscara verdadera, proporcionando una medida de similitud entre ambas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jstQKlCLYQvS"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(preds, labels):\n",
        "    \"\"\"\n",
        "    Calcula la exactitud (accuracy) para un batch de predicciones y etiquetas.\n",
        "\n",
        "    Args:\n",
        "        preds (torch.Tensor): Predicciones del modelo (tensor de tamaño [1, 1, H, W]).\n",
        "        labels (torch.Tensor): Etiquetas verdaderas (tensor de tamaño [1, 1, H, W]).\n",
        "\n",
        "    Returns:\n",
        "        float: Exactitud del batch.\n",
        "    \"\"\"\n",
        "    # Calculamos la accuracy (total de píxeles correctos dividido por el total de píxeles)\n",
        "    correct = (preds == labels).float()\n",
        "    accuracy = correct.sum() / correct.numel()\n",
        "\n",
        "    return accuracy.item()\n",
        "\n",
        "def calculate_dice(preds, labels):\n",
        "    \"\"\"\n",
        "    Calcula el coeficiente Dice para un batch de predicciones y etiquetas.\n",
        "\n",
        "    Args:\n",
        "        preds (torch.Tensor): Predicciones del modelo (tensor de tamaño [1, 1, H, W]).\n",
        "        labels (torch.Tensor): Etiquetas verdaderas (tensor de tamaño [1, 1, H, W]).\n",
        "\n",
        "    Returns:\n",
        "        float: Coeficiente Dice del batch.\n",
        "    \"\"\"\n",
        "    # Calculamos el Dice\n",
        "    intersection = (preds * labels).sum()\n",
        "    union = preds.sum() + labels.sum()\n",
        "    dice = 2. * intersection / union\n",
        "\n",
        "    return dice.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiPP6wtw55Wr"
      },
      "source": [
        "Cargamos el modelo entrenado y lo evaluamos el modelo en el conjunto de prueba y calculamos las métricas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5ppE4LfYvGv"
      },
      "outputs": [],
      "source": [
        "# Cargamos el modelo entrenado\n",
        "model.load_state_dict(torch.load(\"model_best.pth\"))\n",
        "model.to(device)\n",
        "\n",
        "# Evaluación\n",
        "model.eval()\n",
        "total_accuracy = 0.0\n",
        "total_dice = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        preds = (outputs > 0.5).float()\n",
        "\n",
        "        accuracy = calculate_accuracy(preds, labels)\n",
        "        total_accuracy += accuracy\n",
        "\n",
        "        dice = calculate_dice(preds, labels)\n",
        "        total_dice += dice\n",
        "\n",
        "average_accuracy = total_accuracy / len(test_loader)\n",
        "average_dice = total_dice / len(test_loader)\n",
        "\n",
        "print(\"Resultados U-Net:\\n\")\n",
        "print(f\"Accuracy: {average_accuracy:.4f}\")\n",
        "print(f\"Dice: {average_dice:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMS7OwmC4l8x"
      },
      "source": [
        "Para finalizar, visualicemos algunas predicciones del modelo y comparemos con las máscaras verdaderas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7jUC71SZcS1"
      },
      "outputs": [],
      "source": [
        "# Cantidad de ejemplo a visualizar\n",
        "num_samples_to_display = 3\n",
        "num_samples_to_display = min(num_samples_to_display, len(test_loader.dataset))\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, labels) in enumerate(test_loader):\n",
        "\n",
        "        if i >= num_samples_to_display:\n",
        "            break\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        image = inputs[0].cpu().squeeze().numpy()\n",
        "        true_mask = labels[0].cpu().squeeze().numpy()\n",
        "        pred_mask = outputs[0].cpu().squeeze().numpy()\n",
        "        pred_mask = (pred_mask > 0.5).astype(np.uint8)\n",
        "\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
        "        axs[0].imshow(image, cmap=\"gray\")\n",
        "        axs[0].set_title(\"Imagen original\")\n",
        "        axs[0].axis('off')\n",
        "\n",
        "        axs[1].imshow(true_mask, cmap=\"gray\")\n",
        "        axs[1].set_title(\"Máscara verdadera\")\n",
        "        axs[1].axis('off')\n",
        "\n",
        "        axs[2].imshow(pred_mask, cmap=\"gray\")\n",
        "        axs[2].set_title(\"Máscara predicha\")\n",
        "        axs[2].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHUT6g6cZZjo"
      },
      "source": [
        "## **Entregable**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20w6KgR64fd8"
      },
      "source": [
        "Completar el código y analizar los resultados obtenidos."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}